{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in data and cleaning up unnecessary columns.\n",
    "truth_seeker = pd.read_csv(\"TruthSeeker2023/Features_For_Traditional_ML_Techniques.csv\")\n",
    "truth_seeker = truth_seeker.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all features.\n",
    "truth_seeker_features = truth_seeker.drop(columns=[\"majority_target\", \"statement\", \"BinaryNumTarget\", \"tweet\", \"embeddings\"])\n",
    "\n",
    "# Extracting label column for data.\n",
    "truth_seeker_output = truth_seeker[\"BinaryNumTarget\"]\n",
    "\n",
    "# Split the data into training and testing sets (e.g., 75% train, 25% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    truth_seeker_features,\n",
    "    truth_seeker_output,\n",
    "    test_size=0.25,  # Adjust the test_size as needed\n",
    "    random_state=100  # Set a random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create an instance of the StandardScaler.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale (normalize) the features in the training and testing sets.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Models\n",
    "\n",
    "When doing machine learning, one of the most important aspects to consider is what model to fit your data with. A model is the algorithm being used to draw conclusions about the data. In each model, you give it a certain amount of data with the correct answers for the classification (referred to as the training data set) and then give the same model a different set of the same data (referred to as the testing data set) in order to evaluate how well it performs. Each model will have its own advantages and drawbacks depending on its specific methodology and how it interacts with the data. In order to verify the results discussed by Suhaib Kh. Hamed, Mohd Juzaiddin Ab Aziz, and Mohd Ridzwan Yaakub in their paper \"A Review of Fake News Detection Models: Highlighting the Factors Affecting Model Performance and the Prominent Techniques Used\" I chose to test five different kinds of models with a range of different metrics. In the paper, they talk about both machine-learning based and deep-learning based approaches. Deep-learning approaches focus more on neural networks and attempt to mimic the way the human brain works. For the scope of this project, I chose to only work with machine-learning based models. Additionally this paper did not use the same dataset or specify the features used so I am focusing on exploring the overall trends. Another paper, \"The Largest Social Media Ground-Truth Dataset for Real/Fake Content: TruthSeeker\" by Sajjad Dadkhah, Xichen Zhang, Alexander Gerald Weismann, Amir Firouzi, and Ali A. Ghorbani, looks at the specific dataset I'm using so I'll be using their results for direct comparisons.\n",
    "\n",
    "The five models I chose to test are the Decision Tree, Gaussian Naive Bayes, K Nearest Neighbors, Random Forest, and Support Vector classifiers.\n",
    "\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "Decision Trees are one of the most versatile models for text classification. They work by inferring rules from the data features. For example, if we are trying to predict the color of a ball and in the training data it shows that when a ball is thrown more than 50m it's always blue, the decision tree would infer that for any ball that travels more than 50m, its color will be blue. Depending on the number of features given, a decision tree creates a tree-like structure where it makes a decision at each node. For text classification, Decision trees have been used to identify discriminatory language or patterns for sorting texts into categories. Since the essence of fake news detection is sorting texts into a real and a fake category, a Decision Tree is a good model to test. There are a few other advantages to decision trees. They tend to be easier to understand and interpret which sheds more light on the model's process. They are also able to handle non-linear data very well since they don't assume a particular distribution in the data. That being said, when looking at texts with a large number of features such as the Truth Seeker dataset, the model tends to overfit frequently since small changes in the data can lead to completely different tree structures.\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes is a probabilistic model that is an extension of a larger category of models called Naive Bayes models. Naive Bayes (NB) methods are an implementation of the Bayes theorem with the assumption that there is conditional independence between all the features. This means that if the value of one feature changes it won't affect the value of any other features. The most common applications of NB models are in classifying documents, filtering, prediction, etc. So long as the assumption of conditional independence isn't severely violated Gaussian NB is considered a simple but powerful algorithm. It's well suited for larger datasets because its simplicity makes it computationally efficient. However, its simplicity also means that it isn't as effective at identifying complex relationships in the data.\n",
    "\n",
    "### K Nearest Neighbors\n",
    "\n",
    "K Nearest Neighbors (KNN) is a nearest neighbors based learning algorithm. This means that this model bases its decision based on the K closest data points. KNN works on the assumption that similar points will be grouped together and can be used for regression or classification problems. KNN is also a non-parametric algorithm which means it's considered good to use when there is an irregular boundary for classification. Non-parametric simply means that it doesn't assume any kind of underlying mathematical model unlike models like linear regression which assumes a linear relationship in the data. KNN stores the training data in its memory rather than going through an actual training period, which makes it easier to implement, but it also means that the majority of computational cost occurs when it is actually making a classification, leading to computational inefficiency and high memory requirements. This is also a model that's very sensitive to noise in the data because it is based on distance and irrelevant features can distort the distance between data points.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Random Forest (RF) is an ensemble learning method, which means that rather than constructing one model which handles all the data, RF constructs multiple decision trees during its training to create a more robust model. Decision trees in general are known to be prone to overfitting the data and have high variance but RF corrects this in a few different ways. First, the overall model is constructed from many different decision trees which are all trained on different subsets of the training data. Second, RF intentionally adds randomness into its training for this purpose. It does this in two different ways: training on random subsets of features and evaluating multiple splits at every node. By doing these things, RF is able to mitigate the tendency to overfit to individual data points that is often seen with decision trees. This process tends to significantly increase the performance of the model. Since RF is a very robust model it's effective in classifying datasets with a large number of features, which has been particularly useful in this dataset since there are over 50 features. However, the tradeoff of having a large number of trees involved in the algorithm is that it's considered a bit of a \"black-box\" algorithm and doesn't have the same level of interpretability that an individual decision tree might have, as well as being a more computationally expensive model since it needs to train all of these trees.\n",
    "\n",
    "### Support Vector\n",
    "\n",
    "Support Vector Classifiers (SVC) are very powerful classifiers that attempt to identify a hyperplane which sorts the different classes in the data. A hyperplane is essentially a decision boundary where one side of the boundary is in one category and the other is in a different category. An SVC is particularly good at handling a large number of features since its method is to transform data into high-dimensional spaces and search for the decision boundary there. When it comes to giving its decisions, it's quite computationally efficient as it only holds onto a subset of training points. However, its performance is easily affected by the choice of kernel function and the sensitivity to parameter training. This sensitivity means that it's often recommended to scale data so that every feature will play an equal role in decision making rather than a feature that ranges from 0-1000 disproportionately influencing the model compared to a feature that goes from 0-1. It's also another \"black-box\" model where a user might not be able to understand the decision making process easily.\n",
    "\n",
    "As part of my control, I chose to use all the default parameters provided by sklearn for each of the models. I also used the same training and testing data for each of the models, with the only exception being that the SVC model had a scaled version of the data instead. I used K-Fold cross validation in order to mitigate the effects of variability in the data. This is a method where the dataset is divided into K number of folds and each fold is used as a testing set once while the remaining data is the training data. I chose 5 for the number of folds to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store evaluation metrics for each model\n",
    "evaluation_results_data = []\n",
    "\n",
    "# Initialize models\n",
    "tree_model = DecisionTreeClassifier()\n",
    "nb_model = GaussianNB()\n",
    "knn_model = KNeighborsClassifier()\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "models = [tree_model, nb_model, knn_model, rf_model]\n",
    "model_names = [\"Decision Tree\", \"Naive Bayes\", \"KNN\", \"Random Forest\"]\n",
    "\n",
    "# Iterate over models and store evaluation metrics\n",
    "for model, model_name in zip(models, model_names):\n",
    "    metrics_data = train_and_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "    evaluation_results_data.append({\"model_name\": model_name, \"metrics_data\": metrics_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "svc_model = SVC()\n",
    "\n",
    "model_names.append(\"Support Vector Classifier\")\n",
    "models.append(svc_model)\n",
    "\n",
    "# Testing Support Vector Classifier\n",
    "svc_metrics = train_and_evaluate_model(svc_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Add SVC results to the list\n",
    "evaluation_results_data.append({\"model_name\": \"Support Vector Classifier\", \"metrics_data\": svc_metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing all the models I wanted to test, the loop above handles running the testing function on each of the sets and adding all of the metrics to a list to simplify analysis. One thing to note is that the SVC model is not included in the loop. This is due to the scaling mentioned previously so the data inputs are slightly different. The SVC metrics are handled first and added to the list before the loop is run for all of the other models. In terms of evaluation metrics, I looked at the accuracy, F1-score, precision, and recall. These 4 statistics were evaluated both based on the mean of the cross validation trials as well as on a test dataset.\n",
    "\n",
    "In the \"A Review of Fake News Detection Models: Highlighting the Factors Affecting Model Performance and the Prominent Techniques Used\" paper, they mentioned using KNN, RF, NB, and SVM. They concluded that of those four RF and SVM had the highest accuracy which is reflected in my results as well. One thing to note is the difference in computational consumption between RF and SVM. While the RF trials would take somewhere between 30 minutes to an hour to run, the SVM trials took well over 4 hours to complete. If I were to make a recommendation, I would argue that the sheer amount of efficiency that RF has in comparison to SVM would make up for any minor differences in accuracy between the two models. The second paper, \"The Largest Social Media Ground-Truth Dataset for Real/Fake Content: TruthSeeker\", gave a table of the different metrics they got from each model. I'll be primarily focusing on accuracy from this table.\n",
    "\n",
    "![Model Metrics](model_metrics.png) \n",
    "\n",
    "The table doesn't look at an SVM but for the other four I've generally replicated their results. My mean and test accuracies follow the same trend as the paper and for RF, KNN, and the Decision Tree model the accuracies are within a few percentage points. The only one with a significant difference in the Gaussian NB; however, this could be due to the data being imbalanced, as I'll explain in the next section.\n",
    "\n",
    "After analyzing the rest of the calculated metrics, I chose to draw conclusions about each model instead of comparing just the values of one metric for each of them.\n",
    "\n",
    "Decision Tree:\n",
    "\n",
    "This model has an overall moderate performance with the mean statistics all being around 0.6. The test metrics are slightly lower which could indicate some overfitting or variance in the model. A further exploration here could be tuning some of the hyperparameters to handle the potential overfitting.\n",
    "\n",
    "Gaussian NB:\n",
    "\n",
    "The NB model shows the lowest performance across the board especially in the mean metrics. In the test metrics, it still shows a generally lower performance but with an unexpected spike in test precision which could mean the data is imbalanced. One way to diagnose this issue would be to check the distribution in the dataset and depending on its results consider adjusting the sample by oversampling the minority or adjusting weightage during training.\n",
    "\n",
    "KNN:\n",
    "\n",
    "The KNN, similarly to the Decision Tree, shows moderate performance with the mean metrics all being around 0.6. The test metrics are very similar to the mean metrics showing an overall very stable performance.\n",
    "\n",
    "RF:\n",
    "\n",
    "The RF model gives one of the best performances amongst these models with higher mean metrics. The test set metrics are also high meaning that the model generalized well.\n",
    "\n",
    "SVM:\n",
    "\n",
    "The SVM model and RF model have very similar statistics so everything said about the RF model applies here as well. However, the SVM took significantly longer compared to the RF for almost the exact same performance.\n",
    "\n",
    "In summary, the RF model performs the best overall, while the Gaussian NB model has the lowest performance. RF and SVM perform the best among all the tested models, which validates the results of both papers. The other models show similar performances to the statistics given in the second paper and range in their precision, recall, and overall accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for seeing all the metrics in table form as well as plots to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model               Mean Accuracy  Mean Precision Mean Recall    Mean F1 Score  Test Accuracy  Test Precision Test Recall    Test F1 Score  \n",
      "Decision Tree       0.5985         0.6095         0.6074         0.6082         0.5979         0.6072         0.6096         0.6084         \n",
      "Naive Bayes         0.5265         0.5817         0.5383         0.4522         0.5339         0.9452         0.5261         0.6760         \n",
      "KNN                 0.5915         0.6017         0.6045         0.6030         0.5939         0.6066         0.6049         0.6058         \n",
      "Random Forest       0.6902         0.7059         0.6791         0.6909         0.6890         0.6733         0.7078         0.6901         \n",
      "Support Vector Classifier0.6893         0.7074         0.6734         0.6900         0.6853         0.6662         0.7055         0.6853         \n"
     ]
    }
   ],
   "source": [
    "models = [tree_model, nb_model, knn_model, rf_model]\n",
    "model_names = [\"Decision Tree\", \"Naive Bayes\", \"KNN\", \"Random Forest\"]\n",
    "\n",
    "\n",
    "# Print metrics as a table\n",
    "print(\"{:<20}\".format(\"Model\"), end=\"\")\n",
    "for metric_name in [\"Mean Accuracy\", \"Mean Precision\", \"Mean Recall\", \"Mean F1 Score\", \"Test Accuracy\", \"Test Precision\", \"Test Recall\", \"Test F1 Score\"]:\n",
    "    print(\"{:<15}\".format(metric_name), end=\"\")\n",
    "print()  # Newline\n",
    "\n",
    "for row in evaluation_results_data:\n",
    "    model_name = row[\"model_name\"]\n",
    "    metrics_values = [row[\"metrics_data\"][metric] for metric in [\"Mean Accuracy\", \"Mean Precision\", \"Mean Recall\", \"Mean F1 Score\", \"Test Accuracy\", \"Test Precision\", \"Test Recall\", \"Test F1 Score\"]]\n",
    "\n",
    "    print(\"{:<20}\".format(model_name), end=\"\")\n",
    "    for value in metrics_values:\n",
    "        print(\"{:<15.4f}\".format(value), end=\"\")\n",
    "    print()  # Newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "DecisionTreeClassifier()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     metric_values \u001b[39m=\u001b[39m {row[\u001b[39m\"\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m\"\u001b[39m]: row[\u001b[39m\"\u001b[39m\u001b[39mmetrics_data\u001b[39m\u001b[39m\"\u001b[39m][metric_name] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m evaluation_results_data}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Bar plot for each metric\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     ax\u001b[39m.\u001b[39mbar(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         x\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(models)) \u001b[39m+\u001b[39m i \u001b[39m*\u001b[39m bar_width,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         height\u001b[39m=\u001b[39m[metric_values[model_name] \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m models],  \u001b[39m# Use 'models' here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         width\u001b[39m=\u001b[39mbar_width,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         label\u001b[39m=\u001b[39mmetric_name  \u001b[39m# Include metric name in the legend\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Set labels and title for each plot\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m ax\u001b[39m.\u001b[39mset_xticks(np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(models)) \u001b[39m+\u001b[39m (\u001b[39mlen\u001b[39m(metrics_list) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m bar_width \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\n",
      "\u001b[1;32m/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     metric_values \u001b[39m=\u001b[39m {row[\u001b[39m\"\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m\"\u001b[39m]: row[\u001b[39m\"\u001b[39m\u001b[39mmetrics_data\u001b[39m\u001b[39m\"\u001b[39m][metric_name] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m evaluation_results_data}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Bar plot for each metric\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     ax\u001b[39m.\u001b[39mbar(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         x\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(models)) \u001b[39m+\u001b[39m i \u001b[39m*\u001b[39m bar_width,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         height\u001b[39m=\u001b[39m[metric_values[model_name] \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m models],  \u001b[39m# Use 'models' here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         width\u001b[39m=\u001b[39mbar_width,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         label\u001b[39m=\u001b[39mmetric_name  \u001b[39m# Include metric name in the legend\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Set labels and title for each plot\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/madans2984/Fall23/scicomp-p3-fake-news-detection/models.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m ax\u001b[39m.\u001b[39mset_xticks(np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(models)) \u001b[39m+\u001b[39m (\u001b[39mlen\u001b[39m(metrics_list) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m bar_width \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: DecisionTreeClassifier()"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the metric categories\n",
    "mean_metrics = [\"Mean Accuracy\", \"Mean Precision\", \"Mean Recall\", \"Mean F1 Score\"]\n",
    "test_metrics = [\"Test Accuracy\", \"Test Precision\", \"Test Recall\", \"Test F1 Score\"]\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.2\n",
    "\n",
    "for metric_category, metrics_list in zip([\"Mean Metrics\", \"Test Metrics\"], [mean_metrics, test_metrics]):\n",
    "    # Create a new subplot for each metric category\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Iterate over metric names and create a group of bars for each metric\n",
    "    for i, metric_name in enumerate(metrics_list):\n",
    "        # Extract metric values for each model\n",
    "        metric_values = {row[\"model_name\"]: row[\"metrics_data\"][metric_name] for row in evaluation_results_data}\n",
    "\n",
    "        # Bar plot for each metric\n",
    "        ax.bar(\n",
    "            x=np.arange(len(model_names)) + i * bar_width,\n",
    "            height=[metric_values[model_name] for model_name in model_names],  # Use 'model_names' here\n",
    "            width=bar_width,\n",
    "            label=metric_name  # Include metric name in the legend\n",
    "        )\n",
    "\n",
    "    # Set labels and title for each plot\n",
    "    ax.set_xticks(np.arange(len(model_names)) + (len(metrics_list) - 1) * bar_width / 2)\n",
    "    ax.set_xticklabels(model_names)  # Use 'model_names' here\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_title(f'{metric_category} for Different Models')\n",
    "    ax.legend()  # Show legend for metric names\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect I was interested in exploring was how to determine where a model fails. Sometimes there will be a pattern to when a model returns a false positive or negative and knowing that information can help fine-tune an algorithm. I decided to attempt to do this by hand by examining when the model returned incorrectly and the features associated to see if I was able to find a common error between the data points. While I have some starter code for it which does print out all of the information needed to do this analysis, I wasn't able to actually draw any conclusions on it due to the sheer amount of data and features. This is likely not the best way to look for these error patterns so a future exploration could be looking more into the best way to parse through this data. A first step would be to finetune the overall model first so that it would be using a few less features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Identify indices of false positives and false negatives in the predictions\n",
    "false_positive_indices = np.where((y_pred == 1) & (y_test == 0))[0]\n",
    "false_negative_indices = np.where((y_pred == 0) & (y_test == 1))[0]\n",
    "\n",
    "# Create DataFrames to store false positive and false negative examples\n",
    "false_positive_df = pd.DataFrame(columns=[\"Example\", \"Predicted\", \"Actual\", \"Features\"])\n",
    "false_negative_df = pd.DataFrame(columns=[\"Example\", \"Predicted\", \"Actual\", \"Features\"])\n",
    "\n",
    "# Adding features for false positives in DataFrame\n",
    "for idx in false_positive_indices:\n",
    "    if 0 <= idx < len(X_test):\n",
    "        false_positive_df = pd.concat([false_positive_df, pd.DataFrame({\n",
    "            \"Example\": [f\"False Positive Example {idx}\"],\n",
    "            \"Predicted\": [y_pred[idx]],\n",
    "            \"Actual\": [y_test.iloc[idx]],\n",
    "            \"Features\": [str(X_test.iloc[idx])]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Adding features for false negatives in DataFrame\n",
    "for idx in false_negative_indices:\n",
    "    if 0 <= idx < len(X_test):\n",
    "        false_negative_df = pd.concat([false_negative_df, pd.DataFrame({\n",
    "            \"Example\": [f\"False Negative Example {idx}\"],\n",
    "            \"Predicted\": [y_pred[idx]],\n",
    "            \"Actual\": [y_test.iloc[idx]],\n",
    "            \"Features\": [str(X_test.iloc[idx])]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Display false positive DataFrame\n",
    "print(\"\\nFalse Positives:\")\n",
    "print(false_positive_df)\n",
    "\n",
    "# Display false negative DataFrame\n",
    "print(\"\\nFalse Negatives:\")\n",
    "print(false_negative_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
